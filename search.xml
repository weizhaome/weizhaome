[{"title":"机器翻译实习总结","url":"https://weizhaome.github.io/2019/07/05/机器翻译实习总结/","content":"<h1 id=\"机器翻译（中英）实习总结\"><a href=\"#机器翻译（中英）实习总结\" class=\"headerlink\" title=\"机器翻译（中英）实习总结\"></a>机器翻译（中英）实习总结</h1><p>2019年5月-7月参与机器翻译项目，接下来将对这两个月的收获进行总结。</p>\n<h2 id=\"Transform模型的理解\"><a href=\"#Transform模型的理解\" class=\"headerlink\" title=\"Transform模型的理解\"></a>Transform模型的理解</h2><blockquote>\n<p>谷歌提出的transformer模型，使用self-attention层和全连接层实现了很好的机器翻译BLEU结果，没有采用传统的RNN/LSTM/GRU结构，解决了长依赖和训练并行度问题，降低了计算的复杂度。关于论文的详细讲解可以参考：<a href=\"https://www.jianshu.com/p/ef41302edeef等的讲解。\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/ef41302edeef等的讲解。</a></p>\n<ul>\n<li><strong>Attention机制</strong><br><strong> 简介:</strong><br>在传统的encoder-decoder模型中，对于句子对&lt;X,Y&gt;,，我们输入给定句子X=&lt;x1,x2…xm&gt; 经过encoder进行编码后转化为中间语义表示为C=F(x<sub>1</sub>,x<sub>2</sub>…x<sub>m</sub>)，解码器decoder的任务是根据句子的中间语义表示C和之前已经生成的历史信息y<sub>1</sub>,y<sub>2</sub>…y<sub>i-1</sub>)来生成i时刻要生成的单词yi=G(C,y<sub>1</sub>,y<sub>2</sub>…y<sub>i-1</sub>)。<br>这样的框架因为对于预测每一个yi对应的语义编码C都是一样的，无法完全表示整个序列的信息，而且先输入的内容携带的信息会被后面的信息覆盖掉。attention机制将用来解决该问题，在预测时不仅关注全局语义编码向量C，而且增加‘注意力范围’，来表示接下来输出词时候要重点关注输入序列的哪些部分。<br><img src=\"/images/nmt/Attention.png\" alt=\"attention\"><br>此时生成目标句子单词的过程就成了下面的形式:<br>y<sub>1</sub>=f1(C<sub>1</sub>)<br>y<sub>2</sub>=f1(C<sub>2</sub>,y<sub>1</sub>)<br>y<sub>3</sub>=f1(C<sub>3</sub>,y<sub>1</sub>,y<sub>2</sub>)<br><strong>计算过程：</strong>1、计算encoder每个结点与decoderd当前结点的Score值：<br>[s1,s2,s3,s4]=softmax(dot([x1,x2,x3,x4],yi-1))<br>2、当前结点的语义编码为Ci=s1*x1+s2*x2+s3*x3+s4*x4</li>\n<li><strong>self-attention机制</strong><br>self-attention是attention机制的一种，attention是输入输出的权重，而self-attention是自己对自己的权重，计算公式如下图：<br><img src=\"/images/nmt/self-attention.png\" alt=\"self-attention\"><br>Q(queries)，K (keys) and V(Values)， 其中 Key and values 一般对应同样的 vector， K=V 而Query vecotor  是对应目标句子的 word vector。</li>\n<li><strong>论文代码的理解</strong><br>所查看的代码使用tensorflow框架，其各个函数的调用以及参数传递如下图：<br><img src=\"/images/nmt/transformer-function.png\" alt=\"transformer\"><br>这里使用了TensorFlow中的高级API：Estimator、Experiment,具体讲解可以参考：<a href=\"https://www.jiqizhixin.com/articles/2017090901\" target=\"_blank\" rel=\"noopener\">https://www.jiqizhixin.com/articles/2017090901</a></li>\n</ul>\n</blockquote>\n<p><img src=\"/images/nmt/API.png\" alt=\"API\">  </p>\n<h2 id=\"shell命令总结\"><a href=\"#shell命令总结\" class=\"headerlink\" title=\"shell命令总结\"></a>shell命令总结</h2><p>语料库中包含两千多万条训练数据，经过了一系列的过滤得到一个干净整洁的训练预料。在过程中主要用shell对其进行处理，提高处理的速度和效率。接下来将对过程中使用到的shell命令进行总结。</p>\n<ul>\n<li><strong>awk</strong><br>awk用来对文本进行较复杂格式处理,<a href=\"https://www.cnblogs.com/silence-hust/p/4534606.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/silence-hust/p/4534606.html</a> 中对awk做了较为详细的汇总。<blockquote>\n<p>1、统计英文文本中每行的单词个数可以用命令：<br>awk -F ‘ ‘ ‘BEGIN{print “count”, “lineNum”}{print NF “\\t” NR}’ filename 其中，-F 表示分隔符，NF是awk的内置函数表示读取记录的域的个数，NR表示已经读取的记录数，文件的每行相当于记录，文件的每个字段，也就是每个单词相当于记录的域。其中BEGIN后面所跟的命令只会在遍历文件前执行一次，END命令也会在遍历完文件后执行一次。<br>2、删除英文文本中单词数量少于5的行可以用命令：<br>awk -F ‘ ‘ ‘{if (NF &lt; 5) {next} {print}}’ filename 其中，if会用来做判断，如果满足条件将会调用{next}直接跳到下一行，如果不满足会执行{print}输出当前行。<br>3、awk ‘!a[$0]++’ filename 将会去除文件中重复的行，’!’表示非，$0表示当前行，a[$0]以当前行为数据下标建立数组a,a[$0]++表示给数组a赋值，a[$0]+=1。详细解释可以参照：<a href=\"https://www.cnblogs.com/irockcode/p/7044646.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/irockcode/p/7044646.html</a>  </p>\n</blockquote>\n</li>\n<li><strong>grep</strong><br>grep更适合单纯的查找和匹配文本， <a href=\"https://blog.csdn.net/liupeifeng3514/article/details/79880878\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/liupeifeng3514/article/details/79880878</a> 中对grep命令做了详细的介绍。<blockquote>\n<p>1、统计出现文本中出现某种模式n词以上的行：grep -E ‘(pattern){n,}’ filename 其中-E表示使用正则表达式，(pattern)是要找的某种模式，用正则表达式进行匹配，{n,}表示连续出现n次以上。<br>2、grep -f file1 file2 过滤掉文件2中包含文件1的内容。 如果添加 -v 可以实现反向输出，即可以输出两个文件重复的行。</p>\n</blockquote>\n</li>\n<li><strong>shell效率问题</strong><br>因为数据量在千万级，所以一个好的处理策略要比处理工具更加重要，本次项目是中-英的机器翻译，所以在对数据筛选时要同时兼顾两个文件，即源端数据文本和目标端数据文本，在去重的过程中一开始的想法是将源段出现重复和行号找出后保存在文件中，之后删除目标段对应的行，但速度会很慢。<br>后来利用paste 先将两个文件用TAB作为分隔符左右拼接起来：<br>paste -d\\t file1 file2 &gt; tempfile<br>之后利用awk去除重复行：<br>awk -F ‘\\t’ ‘!a[$1]++’ tempfile &gt; uniqfile 这里的$1表示第一列即源段文本。<br>最后用cut将其分开。<br>后来看到了<a href=\"https://blog.csdn.net/dubendi/article/details/79103170，关于如何提高shell效率的博客。\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/dubendi/article/details/79103170，关于如何提高shell效率的博客。</a></li>\n<li><strong>shell输出重定向</strong><br><img src=\"/images/nmt/stdout.png\" alt=\"stdout\"><br>在此次项目中原始训练集在一个文件中，按照一行源段，一行目标端，一行空格来放。所以首要任务是将其分成两个文件，这里利用重定向写了一小段shell脚本：<pre><code class=\"#!/usr/bin/perl\">$n=0;\nwhile(&lt;&gt;){\nchomp();\nif($n%3==0){print STDOUT &quot;$_\\n&quot;;}\nif($n%3==1){print STDERR &quot;$_\\n&quot;;}\n$n+=1;\n}\n</code></pre>\n初始化变量n,用来表示读取的是源段还会目标端的行，按行读取文件，当是空行时用chomp()删除改行，如果不是判断n是否能被3整除，如果可以的话用标准输出将其输出，如果不是的话用标准错误输出将其输出，最后在命令行运行该脚本：$./vcut.pl sg-mt.txt 2&gt; train.en 1&gt; train.zh 实现将两中文本分开。</li>\n</ul>\n","categories":[],"tags":[]},{"title":"常见排序算法总结","url":"https://weizhaome.github.io/2019/04/16/常见排序算法总结/","content":"<h1 id=\"常见排序算法总结\"><a href=\"#常见排序算法总结\" class=\"headerlink\" title=\"常见排序算法总结\"></a>常见排序算法总结</h1><h2 id=\"冒泡排序\"><a href=\"#冒泡排序\" class=\"headerlink\" title=\"冒泡排序\"></a>冒泡排序</h2><h3 id=\"基本思想：\"><a href=\"#基本思想：\" class=\"headerlink\" title=\"基本思想：\"></a>基本思想：</h3><blockquote>\n<p>比较序列中相邻两个数据的大小，如果两个数的排序方式和目标相反则调换两个数的位置。</p>\n</blockquote>\n<h3 id=\"代码实现-python-：\"><a href=\"#代码实现-python-：\" class=\"headerlink\" title=\"代码实现(python)：\"></a>代码实现(python)：</h3><pre><code>def bubble_sort(mylist):\n    n = len(mylist)\n    for j in range(n-1):\n        for i in range(0,n-1-j):\n            count = 0\n            if mylist[i]&gt;mylist[i+1]:\n                mylist[i], mylist[i+1]=mylist[i+1],mylist[i]\n                count+=1\n         if count == 0:\n             break\n</code></pre><h3 id=\"时间复杂度：\"><a href=\"#时间复杂度：\" class=\"headerlink\" title=\"时间复杂度：\"></a>时间复杂度：</h3><blockquote>\n<p>最好情况：正序有序，则只需要比较n次不需要交换位置，则时间复杂度为O(n)。</p>\n</blockquote>\n<blockquote>\n<p>最坏情况：逆序有序，则需要比较(n-1)+(n-2)+……+1，则时间复杂度为O(n*n)。</p>\n</blockquote>\n<blockquote>\n<p>平均情况：O(n*n)。</p>\n</blockquote>\n<h3 id=\"稳定性：\"><a href=\"#稳定性：\" class=\"headerlink\" title=\"稳定性：\"></a>稳定性：</h3><blockquote>\n<p>稳定性是指序列中两个相等的数在经过排序算法后两个数的位置是否会发生变化，因为冒泡排序当相邻两个数相等时不会交换他们的位置，所以冒泡算法是稳定的。</p>\n</blockquote>\n<h2 id=\"插入排序\"><a href=\"#插入排序\" class=\"headerlink\" title=\"插入排序\"></a>插入排序</h2><h3 id=\"基本思想：-1\"><a href=\"#基本思想：-1\" class=\"headerlink\" title=\"基本思想：\"></a>基本思想：</h3><blockquote>\n<p>序列中的数往前依次与其前面的数作比较寻找该数的合适位置，直至最后一个元素。</p>\n</blockquote>\n<h3 id=\"代码实现-python\"><a href=\"#代码实现-python\" class=\"headerlink\" title=\"代码实现(python)\"></a>代码实现(python)</h3><pre><code>def insertion_sort(mylist):\n    n = len(mylist)\n    for i in range(1,n):\n        for j in range(i,0,-1):\n            if mylist[j]&gt;mylist[j-1]:\n                mylist[j],mylist[j-1]=mylist[j-1],mylist[j]\n                else:\n                    break\n</code></pre><h3 id=\"时间复杂度：-1\"><a href=\"#时间复杂度：-1\" class=\"headerlink\" title=\"时间复杂度：\"></a>时间复杂度：</h3><blockquote>\n<p>最好情况：正序有序，比较n次不需要交换位置，时间复杂度为O(n)。</p>\n</blockquote>\n<blockquote>\n<p>最坏情况：逆序有序，比较n个元素需要跟前面的n-1个元素比较，时间复杂度为O(n^2)。</p>\n</blockquote>\n<blockquote>\n<p>平均情况：O(n^2)。</p>\n</blockquote>\n<h3 id=\"稳定性：-1\"><a href=\"#稳定性：-1\" class=\"headerlink\" title=\"稳定性：\"></a>稳定性：</h3><blockquote>\n<p>当两个元素比较时如果两个元素相等则不需要移动，所以插入排序是稳定的。</p>\n</blockquote>\n","categories":[],"tags":["sorts"]},{"title":"解释性语言和编译型语言的区别","url":"https://weizhaome.github.io/2019/03/25/解释性语言和编译型语言的区别/","content":"<h1 id=\"解释性语言和编译型语言的区别\"><a href=\"#解释性语言和编译型语言的区别\" class=\"headerlink\" title=\"解释性语言和编译型语言的区别\"></a>解释性语言和编译型语言的区别</h1><blockquote>\n<p>在高级语言编程中需要将高级语言翻译成计算机能够理解的机器语言。其中的翻译又分为两种方式：一种是编译，一种是解释。</p>\n<ul>\n<li>解释性语言翻译的过程是在运行的过程中进行的。Java是编译-解释型，Javac命令首先将程序源码的.java文件编译成 .class字节码文件，之后Java虚拟机边读取边翻译边执行。</li>\n<li>编译型语言需要在运行之前有单独的翻译过程。C语言的编译过程包括：C源程序–&gt;预编译处理(.c)–&gt;编译、优化程序(.s、.asm)–&gt;汇编程序(.obj、.o、.a、.ko)–&gt;链接程序(.exe、.elf、.axf等)</li>\n</ul>\n</blockquote>\n","categories":[],"tags":["Java/C"]},{"title":"向github提交本地仓库","url":"https://weizhaome.github.io/2019/03/09/向gitHub提交本地仓库/","content":"<h2 id=\"如果需要将本地生成的工程提交到github相应的仓库中的步骤如下：\"><a href=\"#如果需要将本地生成的工程提交到github相应的仓库中的步骤如下：\" class=\"headerlink\" title=\"如果需要将本地生成的工程提交到github相应的仓库中的步骤如下：\"></a>如果需要将本地生成的工程提交到github相应的仓库中的步骤如下：</h2><blockquote>\n<p>1、在github上新建立相应的仓库名称，比如：blog。</p>\n</blockquote>\n<blockquote>\n<p>2、cd到本地工程的根目录下。</p>\n</blockquote>\n<blockquote>\n<p>3、使用命令：git init 初始化本地仓库。</p>\n</blockquote>\n<blockquote>\n<p>4、使用命令：git add ××× 添加要提交到github远程仓库的文件或者文件<br>夹，如果全部提交可以使用：git add . 。</p>\n</blockquote>\n<blockquote>\n<p>5、git commit -m “备注”。</p>\n</blockquote>\n<blockquote>\n<p>6、git remote add origin <a href=\"https://github.com/weizhaome/blog\" target=\"_blank\" rel=\"noopener\">https://github.com/weizhaome/blog</a> 建立远程仓库链接。</p>\n</blockquote>\n<blockquote>\n<p>7、最后使用 git push -u origin master 就可以将本地工程提交到github上了。</p>\n</blockquote>\n<h2 id=\"遇见的问题：\"><a href=\"#遇见的问题：\" class=\"headerlink\" title=\"遇见的问题：\"></a>遇见的问题：</h2><blockquote>\n<p>1、当github远程仓库中存在本地仓库所不存在的文件时需要先将两部分进行合并：git pull –rebase origin master 之后再执行push操作。</p>\n</blockquote>\n<blockquote>\n<p>2、Push 一直提示 “Permission denied (publickey) “ , 这个可能是由于你的没有目标仓库和分支的权限，导致无法更新数据。</p>\n<ul>\n<li>确认 push 方式，如果是 SSH 方式请检查你的 SSH 公钥是否正确（如果您有多个私钥，请使用 ssh-add 命令来指定默认使用的私钥）； HTTPS 方式检查密码及用户名是否正确。</li>\n<li>对目标分支是否有写权限。</li>\n<li>善用 搜索。</li>\n</ul>\n</blockquote>\n","categories":[],"tags":[]},{"title":"Spark reduceByKey使用时遇到的问题","url":"https://weizhaome.github.io/2018/09/27/Spark-reduceByKey使用时遇到的问题/","content":"<h1 id=\"reduceByKey-func-不会处理单一值的RDD\"><a href=\"#reduceByKey-func-不会处理单一值的RDD\" class=\"headerlink\" title=\"reduceByKey(func)不会处理单一值的RDD\"></a>reduceByKey(func)不会处理单一值的RDD</h1><ul>\n<li><p>reduceByKey(func)的功能是，使用func函数合并具有相同键的值。比如，reduceByKey((a,b) =&gt; a+b)，有四个键值对(“spark”,1)、(“spark”,2)、(“hadoop”,3)和(“hadoop”,5)，对具有相同key的键值对进行合并后的结果就是：(“spark”,3)、(“hadoop”,8)。</p>\n</li>\n<li><p><strong>但是，当reduceByKey(func)中的func函数对值进行split操作时，对于只有唯一值的RDD不会被split。下面使用创建的简单数据进行说明:</strong></p>\n</li>\n</ul>\n<blockquote>\n<p>统计原始数据中相同姓名都出现了哪些数字，原始数据如下图所示：</p>\n</blockquote>\n<p><img src=\"/images/reduceByKey-question/rawdata.jpg\" alt=\"github\"></p>\n<p>可以看到数据中姓名后面的数字中含有”\\t”。</p>\n<blockquote>\n<p>读取文件数据并将其map成键值对的形式，得到的结果如下图所示：</p>\n</blockquote>\n<p><img src=\"/images/reduceByKey-question/FirstMap.jpg\" alt=\"github\"></p>\n<p>我们看到数据读入后”\\t”变为”\\\\t”,姓名成为键，后面的数字成为了值。</p>\n<blockquote>\n<p>使用reduceByKey合并相同姓名后面出现的数字，在这个过程中使用split(“\\t”)对值进行切分。</p>\n</blockquote>\n<p><img src=\"/images/reduceByKey-question/reduceByKey.jpg\" alt=\"github\"></p>\n<p>从结果中可以看出，“xiaoguniang” 的值并没有被split.reduceByKey中不能对唯一值的RDD进行后面的func。也就是说当只有一个值的RDD出现时reduceByKey是不对其进行处理的。</p>\n<blockquote>\n<p>为此，为了达到想要的效果我们对代码进行改进，使用mapValue()对Value进行预处理，之后再进行reduceByKey()。</p>\n</blockquote>\n<p><img src=\"/images/reduceByKey-question/improve.jpg\" alt=\"github\"></p>\n","categories":[],"tags":[]},{"title":"Spark Streaming","url":"https://weizhaome.github.io/2018/09/18/Spark-Streaming/","content":"<h1 id=\"Spark-Streaming\"><a href=\"#Spark-Streaming\" class=\"headerlink\" title=\"Spark Streaming\"></a>Spark Streaming</h1><blockquote>\n<p>Spark Streaming是构建在Spark上的实时计算框架，它扩展了Spark处理大规模流式数据的能力。Spark Streaming可结合批处理和交互查询，适合一些需要对历史数据和实时数据进行结合分析的应用场景。Spark Streaming最主要的抽象是DStream（Discretized Stream，离散化数据流），表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按照时间片（如1秒）分成一段一段的DStream，每一段数据转换为Spark中的RDD，并且对DStream的操作都最终转变为对相应的RDD的操作。</p>\n</blockquote>\n<h2 id=\"Spark-Streaming程序基本步骤\"><a href=\"#Spark-Streaming程序基本步骤\" class=\"headerlink\" title=\"Spark Streaming程序基本步骤\"></a>Spark Streaming程序基本步骤</h2><ol>\n<li>通过创建输入DStream来定义输入源。</li>\n<li>通过对DStream应用转换操作和输出操作来定义流计算。</li>\n<li>用streamingContext.start()来开始接收数据和处理流程。</li>\n<li>通过streamingContext.awaitTermination()方法来等待处理结束（手动结束或因为错误而结束）。</li>\n<li>可以通过streamingContext.stop()来手动结束流计算进程。<h2 id=\"RDD队列流\"><a href=\"#RDD队列流\" class=\"headerlink\" title=\"RDD队列流\"></a>RDD队列流</h2><blockquote>\n<p>在调试Spark Streaming应用程序的时候，我们可以使用streamingContext.queueStream(queueOfRDD)创建基于RDD队列的DStream。</p>\n</blockquote>\n</li>\n</ol>\n<p>下面是参考Spark官网的QueueStream程序设计的程序，每隔1秒创建一个RDD，Streaming每隔2秒就对数据进行处理。</p>\n<p><img src=\"/images/RDD队列流.png\" alt=\"Github\" title=\"RDD队列流\"></p>\n<p><img src=\"/images/RDD队列流-2.png\" alt=\"Github\" title=\"结果\"></p>\n<p>Spark也支持从兼容HDFS API的文件系统读取数据和通过Socket端口监听并接收数据，创建数据流。<br><img src=\"/images/RDD文件流.png\" alt=\"Github\" title=\"RDD文件流\"></p>\n","categories":[],"tags":[]},{"title":"Spark","url":"https://weizhaome.github.io/2018/09/17/Hive/","content":"<h1 id=\"Spark-笔记\"><a href=\"#Spark-笔记\" class=\"headerlink\" title=\"Spark 笔记\"></a>Spark 笔记</h1><ul>\n<li><h2 id=\"Spark架构设计\"><a href=\"#Spark架构设计\" class=\"headerlink\" title=\"Spark架构设计\"></a>Spark架构设计</h2><strong>基本概念：</strong></li>\n</ul>\n<p>1.RDD:弹性分布式数据集，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型。<br>2.Executor:运行在工作节点（Worker Node）上的一个进程，负责运行任务，并为应用程序存储数据。<br>3.DAG:有向无环图（Driected Acyclic Graph）,反映RDD之间的依赖关系（宽依赖和窄依赖）。<br>4.应用：用户编写的Spark应用程序。<br>5.任务：运行在Exector的工作单元。<br>6.作业：一个作业包含多个RDD及作用在相应RDD上的各种操作。<br>7.阶段：是作业的基本调度单位，一个作业会分为多组任务，每组任务被成为”阶段“或者“任务集”。</p>\n<p><strong>架构图</strong></p>\n<p><img src=\"/images/Spark-1.png\" alt=\"Github\" title=\"Spark\"></p>\n<p><strong>运行基本流程</strong></p>\n<p>1.当一个Spark应用被提交时，首先为这个应用构建起基本的运行环境，即由任务控制节点（Driver）创建一个SparkContext,由SparkContext负责和资源管理器（Cluster Manager）的通信以及资源的申请、任务的分配和监控等。SparkContext会向资源管理器注册并申请运行Executor的资源。<br>2.资源管理器为Executor分配资源，并启动Executor进程，Executor运行情况将随着“心跳”发送给资源管理器。<br>3.SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAG调度器进行解析，将DAG图分解为多个“阶段”（每个阶段就是一个“任务集”），并且计算出各个阶段之间的依赖关系，然后把一个个“任务集”提交给底层的任务调度器进行处理，Executor向SparkContext申请任务，任务调度器将任务分发给Executor运行。同时，SparkContext将应用程序代码发给Executor。<br>4.任务在Executor上运行，把执行结构反馈给任务调度器，然后反馈给DAG调度器，运行完毕后写入数据并释放所有资源。</p>\n<p>参照：<a href=\"http://dblab.xmu.edu.cn/blog/1709-2/\" target=\"_blank\" rel=\"noopener\">http://dblab.xmu.edu.cn/blog/1709-2/</a></p>\n","categories":[],"tags":["Spark"]},{"title":"Hadoop 笔记","url":"https://weizhaome.github.io/2018/09/16/Hadoop-笔记/","content":"<h1 id=\"Hadoop基础\"><a href=\"#Hadoop基础\" class=\"headerlink\" title=\"Hadoop基础\"></a>Hadoop基础</h1><ul>\n<li><h2 id=\"是什么\"><a href=\"#是什么\" class=\"headerlink\" title=\"是什么\"></a>是什么</h2><blockquote>\n<p>Hadoop是Apache开源组织的一个分布式计算开源框架（<a href=\"http://hadoop.apache.org/)，\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/)，</a> 能够实现集群中对海量数据进行分布式计算。</p>\n</blockquote>\n</li>\n<li><h2 id=\"Hadoop架构\"><a href=\"#Hadoop架构\" class=\"headerlink\" title=\"Hadoop架构\"></a>Hadoop架构</h2><img src=\"/images/Hadoop.png\" alt=\"Github\"></li>\n<li>HDFS: 提供对应用程序数据高吞吐量访问的分布式文件系统。</li>\n<li>YARN: 作业调度和集群资源管理的框架</li>\n<li>MapReduce: 基于YARN的大型数据集并行处理系统，实现分布式计算。</li>\n<li>Others:利用YARN的资源管理功能实现其他的数据处理方式。<h3 id=\"HDFS的关键元素\"><a href=\"#HDFS的关键元素\" class=\"headerlink\" title=\"HDFS的关键元素\"></a>HDFS的关键元素</h3></li>\n</ul>\n<ol>\n<li>Block：将一个文件进行分块，通常是64M。一个大文件会被拆分成一个个的块，然后存储于不同的机器。如果一个文件少于Block大小，那么实际占用的空间为其文件的大小。</li>\n<li>NameNode: 保存整个文件系统的目录信息、文件信息及分块信息，这是由唯一一台主机专门保存，当然这台主机如果出错，NameNode就失效了。在Hadoop2.*开始支持activity-standy模式—-如果主NameNode失效，启动备用主机运行NameNode。</li>\n<li>DataNode: 保存具体的block数据,负责数据的读写操作和复制操作,DataNode启动时会向NameNode报告当前存储的数据块信息，后续也会定时报告修改信息,DataNode之间会进行通信，复制数据块，保证数据的冗余性。</li>\n<li>Secondary NameNode：定时与NameNode进行同步（定期合并文件系统镜像和编辑日&amp;#x#x5FD7;，然后把合并后的传给NameNode，替换其镜像，并清空编辑日志，类似于CheckPoint机制），但NameNode失效后仍需要手工将其设置成主机。<h3 id=\"YARN的组件以及架构\"><a href=\"#YARN的组件以及架构\" class=\"headerlink\" title=\"YARN的组件以及架构\"></a>YARN的组件以及架构</h3><h4 id=\"YARN主要由以下几个组件组成：\"><a href=\"#YARN主要由以下几个组件组成：\" class=\"headerlink\" title=\"YARN主要由以下几个组件组成：\"></a>YARN主要由以下几个组件组成：</h4><blockquote>\n<ol>\n<li>ResourceManager：Global（全局）的进程 </li>\n<li>NodeManager：运行在每个节点上的进程</li>\n<li>ApplicationMaster：Application-specific（应用级别）的进程</li>\n</ol>\n<ul>\n<li><em>Scheduler：是ResourceManager的一个组件</em></li>\n<li><em>Container：节点上一组CPU和内存资源</em></li>\n</ul>\n</blockquote>\n</li>\n</ol>\n<ul>\n<li>YARN的架构图及流程</li>\n</ul>\n<p><img src=\"/images/YARN.png\" alt=\"Github\" title=\"YARN\"></p>\n<p>1.客户端程序向ResourceManager提交应用并请求一个ApplicationMaster实例</p>\n<p>2.ResourceManager找到可以运行一个Container的NodeManager，并在这个Container中启动ApplicationMaster实例</p>\n<p>3.ApplicationMaster向ResourceManager进行注册，注册之后客户端就可以查询ResourceManager获得自己ApplicationMaster的详细信息，以后就可以和自己的ApplicationMaster直接交互了</p>\n<p>4.在平常的操作过程中，ApplicationMaster根据resource-request协议向ResourceManager发送resource-request请求</p>\n<p>5.当Container被成功分配之后，ApplicationMaster通过向NodeManager发送container-launch-specification信息来启动Container， container-launch-specification信息包含了能够让Container和ApplicationMaster交流所需要的资料</p>\n<p>6.应用程序的代码在启动的Container中运行，并把运行的进度、状态等信息通过application-specific协议发送给ApplicationMaster</p>\n<p>7.在应用程序运行期间，提交应用的客户端主动和ApplicationMaster交流获得应用的运行状态、进度更新等信息，交流的协议也是application-specific协议</p>\n<p>8.一但应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster向ResourceManager取消注册然后关闭，用到所有的Container也归还给系统<br>详情请见：<a href=\"https://blog.csdn.net/suifeng3051/article/details/49486927\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/suifeng3051/article/details/49486927</a></p>\n<h3 id=\"MapReduce主要过程\"><a href=\"#MapReduce主要过程\" class=\"headerlink\" title=\"MapReduce主要过程\"></a>MapReduce主要过程</h3><p>MapReduce把一个任务拆分成了多个小任务，并把子任务分配到多台计算机上进行工作。最终，每台计算机上的计算结果会被搜集起来并合并成最终的结果。在Map阶段将数据集的键值对映射为另一组键值对。Reduce阶段得到Map的输出，并把具有相同键的数据合并成一个更小的键值对数据集。<br><img src=\"/images/MapReduce.png\" alt=\"Github\" title=\"MapReduce\"></p>\n<ul>\n<li>Input Phase: 使用一个Record Reader将输入文件中的每一条数据转换为键值对的形式，并把这些处理好的数据发送给Mapper。</li>\n<li>Map: Map是是用户自定义的一个函数，此函数接收一系列的键值对数据并对它们进行处理，最后生成0个或多个键值对数据。</li>\n<li>Intermediate Keys: 由mapper生成的键值对数据被称为中间状态的键值对。</li>\n<li>Shuffle and Sort: Reducer任务通常以Shuffle（搅动）和Sort（排序）开始。程序把分好组的键值对数据下载到本机，Reducer会在本机进行运行。这些独立的键值对数据会按照键值进行排序并形成一个较大的数据序列，数据序列中键值相等的键值对数据会被分在相同的一组，这样易于在Reducer任务中进行迭代操作。</li>\n<li>Reducer:Reducer任务把分好组的键值对数据作为输入，并且对每一个键值对都执行Reducer函数。在这个阶段，程序会以不同的方式对数据进行合并、筛选。一旦执行完毕，Reducer会生成0个或多个键值对数据，并提供给最后一个处理步骤。</li>\n<li>Output Phase: 在输出阶段，通过record writer把从Reducer函数输出的键值对数据按照一定的格式写入到文件中。<br>参考：<a href=\"https://www.jianshu.com/p/6162b787a428\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/6162b787a428</a></li>\n</ul>\n","categories":[],"tags":["BigData"]},{"title":"Tags","url":"https://weizhaome.github.io/tag/index.html","content":"<p>Hadoop<br>Spark</p>\n","categories":[],"tags":[]},{"title":"About Me","url":"https://weizhaome.github.io/about/index.html","content":"<ul>\n<li><strong>基本信息</strong></li>\n</ul>\n<blockquote>\n<p>姓名：赵伟  </p>\n</blockquote>\n<blockquote>\n<p>性别：女</p>\n</blockquote>\n<blockquote>\n<p>外语水平：CET6</p>\n</blockquote>\n<blockquote>\n<p>政治面貌：党员</p>\n</blockquote>\n<blockquote>\n<p>邮箱： <a href=\"mailto:zhao_wei@tju.edu.cn\" target=\"_blank\" rel=\"noopener\">zhao_wei@tju.edu.cn</a></p>\n</blockquote>\n<blockquote>\n<p>硕士就读于天津大学（985）计算机技术专业（2017至今）</p>\n</blockquote>\n<blockquote>\n<p>本科就读于东北林业大学（211）软件工程专业（2013-2017）</p>\n</blockquote>\n<ul>\n<li><strong>论文成果</strong></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">时间</th>\n<th style=\"text-align:center\">论文名称</th>\n<th style=\"text-align:center\">排序</th>\n<th style=\"text-align:center\">刊物等级</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">   2017.12</td>\n<td style=\"text-align:center\">UltraPse: A Universal and Extensible Software Platform for Representing Biological Sequences</td>\n<td style=\"text-align:center\">二作</td>\n<td style=\"text-align:center\">SCI三区</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">   2018.09</td>\n<td style=\"text-align:center\">A brief review on software tools in generating Chou’s pseudo-factor representations for all types of biological sequences</td>\n<td style=\"text-align:center\">一作</td>\n<td style=\"text-align:center\">SCI四区</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">   2019.02</td>\n<td style=\"text-align:center\">Predicting protein sub-Golgi locations by combining functional domain enrichment scores with pseudo-amino acid compositions</td>\n<td style=\"text-align:center\">一作</td>\n<td style=\"text-align:center\">SCI四区在投</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><strong>项目经历</strong></li>\n</ul>\n<blockquote>\n<p>1、2019.05-至今，在慧言科技（天津）有限公司实习，参与机器翻译项目，编写shell脚本程序对有千万级数据量的语料库进行清洗，并利用深度学习算法训练和测试机器翻译模型，实现对现有机器翻译模型的优化和升级；</p>\n</blockquote>\n<blockquote>\n<p>2、利用flask框架构建蛋白质亚细胞位点预测网站，用户向网站提交蛋白质序列后，网站将对该蛋白质序列的亚细胞核、亚线粒体、亚叶绿体以及亚高尔基体位点进行预测，并将结果返回给用户。</p>\n<ul>\n<li><strong>所获证书</strong></li>\n</ul>\n</blockquote>\n<blockquote>\n<p>1、2016-2017学年： 获得“三好学生”荣誉称号；“国家励志”奖学金；</p>\n</blockquote>\n<blockquote>\n<p>2、2015-2016学年： 获得“优秀团员”荣誉称号；</p>\n</blockquote>\n<blockquote>\n<p>3、2014-2015学年：“牵手”奖学金；获得校级“优秀学生干部”荣誉称号。</p>\n</blockquote>\n<ul>\n<li><strong>学生干部任职情况</strong></li>\n</ul>\n<blockquote>\n<p>1、2015.09-2016.09 担任东北林业大学勤工助学中心部长，组织东北林业大学家庭经济困难学生技能培训并取得圆满成功；</p>\n</blockquote>\n<blockquote>\n<p>2、2013.09-2017.06 担任寝室长，带领我们寝室成为校“优秀寝室”。</p>\n</blockquote>\n<ul>\n<li><strong> 社会实践经历</strong></li>\n</ul>\n<blockquote>\n<p>1、2018.03-2018.07，在天津大学担任C++课程助教，协助老师完成学生上机实验并在过程中解决学生遇到的问题；</p>\n</blockquote>\n<blockquote>\n<p>2、2015.09-2016.09，在东北林业大学教育超市丹青分店担任收银员，让自己的耐心和细心都有所提高。</p>\n</blockquote>\n","categories":[],"tags":[]}]