[{"title":"向github提交本地仓库","url":"https://weizhaome.github.io/2019/03/09/向gitHub提交本地仓库/","content":"<h2 id=\"如果需要将本地生成的工程提交到github相应的仓库中的步骤如下：\"><a href=\"#如果需要将本地生成的工程提交到github相应的仓库中的步骤如下：\" class=\"headerlink\" title=\"如果需要将本地生成的工程提交到github相应的仓库中的步骤如下：\"></a>如果需要将本地生成的工程提交到github相应的仓库中的步骤如下：</h2><blockquote>\n<p>1、在github上新建立相应的仓库名称，比如：blog。</p>\n</blockquote>\n<blockquote>\n<p>2、cd到本地工程的根目录下。</p>\n</blockquote>\n<blockquote>\n<p>3、使用命令：git inint 初始化本地仓库。</p>\n</blockquote>\n<blockquote>\n<p>4、使用命令：git add ××× 添加要提交到github远程仓库的文件或者文件夹，如果全部提交可以使用：git add . 。</p>\n</blockquote>\n<blockquote>\n<p>5、git commit -m “备注”</p>\n</blockquote>\n<blockquote>\n<p>6、git remote add origin <a href=\"https://github.com/weizhaome/blog\" target=\"_blank\" rel=\"noopener\">https://github.com/weizhaome/blog</a> 建立远程仓库链接。</p>\n</blockquote>\n<blockquote>\n<p>7、git push -u origin master 将本地仓库push到远程仓库</p>\n</blockquote>\n<blockquote>\n<p>8、最后使用 git push -u origin master 就可以将本地工程提交到github上了。</p>\n</blockquote>\n","categories":[],"tags":[]},{"title":"Spark reduceByKey使用时遇到的问题","url":"https://weizhaome.github.io/2018/09/27/Spark-reduceByKey使用时遇到的问题/","content":"<h1 id=\"reduceByKey-func-不会处理单一值的RDD\"><a href=\"#reduceByKey-func-不会处理单一值的RDD\" class=\"headerlink\" title=\"reduceByKey(func)不会处理单一值的RDD\"></a>reduceByKey(func)不会处理单一值的RDD</h1><ul>\n<li><p>reduceByKey(func)的功能是，使用func函数合并具有相同键的值。比如，reduceByKey((a,b) =&gt; a+b)，有四个键值对(“spark”,1)、(“spark”,2)、(“hadoop”,3)和(“hadoop”,5)，对具有相同key的键值对进行合并后的结果就是：(“spark”,3)、(“hadoop”,8)。</p>\n</li>\n<li><p><strong>但是，当reduceByKey(func)中的func函数对值进行split操作时，对于只有唯一值的RDD不会被split。下面使用创建的简单数据进行说明:</strong></p>\n</li>\n</ul>\n<blockquote>\n<p>统计原始数据中相同姓名都出现了哪些数字，原始数据如下图所示：</p>\n</blockquote>\n<p><img src=\"/images/reduceByKey-question/rawdata.jpg\" alt=\"github\"></p>\n<p>可以看到数据中姓名后面的数字中含有”\\t”。</p>\n<blockquote>\n<p>读取文件数据并将其map成键值对的形式，得到的结果如下图所示：</p>\n</blockquote>\n<p><img src=\"/images/reduceByKey-question/FirstMap.jpg\" alt=\"github\"></p>\n<p>我们看到数据读入后”\\t”变为”\\\\t”,姓名成为键，后面的数字成为了值。</p>\n<blockquote>\n<p>使用reduceByKey合并相同姓名后面出现的数字，在这个过程中使用split(“\\t”)对值进行切分。</p>\n</blockquote>\n<p><img src=\"/images/reduceByKey-question/reduceByKey.jpg\" alt=\"github\"></p>\n<p>从结果中可以看出，“xiaoguniang” 的值并没有被split.reduceByKey中不能对唯一值的RDD进行后面的func。也就是说当只有一个值的RDD出现时reduceByKey是不对其进行处理的。</p>\n<blockquote>\n<p>为此，为了达到想要的效果我们对代码进行改进，使用mapValue()对Value进行预处理，之后再进行reduceByKey()。</p>\n</blockquote>\n<p><img src=\"/images/reduceByKey-question/improve.jpg\" alt=\"github\"></p>\n","categories":[],"tags":[]},{"title":"Spark Streaming","url":"https://weizhaome.github.io/2018/09/18/Spark-Streaming/","content":"<h1 id=\"Spark-Streaming\"><a href=\"#Spark-Streaming\" class=\"headerlink\" title=\"Spark Streaming\"></a>Spark Streaming</h1><blockquote>\n<p>Spark Streaming是构建在Spark上的实时计算框架，它扩展了Spark处理大规模流式数据的能力。Spark Streaming可结合批处理和交互查询，适合一些需要对历史数据和实时数据进行结合分析的应用场景。Spark Streaming最主要的抽象是DStream（Discretized Stream，离散化数据流），表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按照时间片（如1秒）分成一段一段的DStream，每一段数据转换为Spark中的RDD，并且对DStream的操作都最终转变为对相应的RDD的操作。</p>\n</blockquote>\n<h2 id=\"Spark-Streaming程序基本步骤\"><a href=\"#Spark-Streaming程序基本步骤\" class=\"headerlink\" title=\"Spark Streaming程序基本步骤\"></a>Spark Streaming程序基本步骤</h2><ol>\n<li>通过创建输入DStream来定义输入源。</li>\n<li>通过对DStream应用转换操作和输出操作来定义流计算。</li>\n<li>用streamingContext.start()来开始接收数据和处理流程。</li>\n<li>通过streamingContext.awaitTermination()方法来等待处理结束（手动结束或因为错误而结束）。</li>\n<li>可以通过streamingContext.stop()来手动结束流计算进程。<h2 id=\"RDD队列流\"><a href=\"#RDD队列流\" class=\"headerlink\" title=\"RDD队列流\"></a>RDD队列流</h2><blockquote>\n<p>在调试Spark Streaming应用程序的时候，我们可以使用streamingContext.queueStream(queueOfRDD)创建基于RDD队列的DStream。</p>\n</blockquote>\n</li>\n</ol>\n<p>下面是参考Spark官网的QueueStream程序设计的程序，每隔1秒创建一个RDD，Streaming每隔2秒就对数据进行处理。</p>\n<p><img src=\"/images/RDD队列流.png\" alt=\"Github\" title=\"RDD队列流\"></p>\n<p><img src=\"/images/RDD队列流-2.png\" alt=\"Github\" title=\"结果\"></p>\n<p>Spark也支持从兼容HDFS API的文件系统读取数据和通过Socket端口监听并接收数据，创建数据流。<br><img src=\"/images/RDD文件流.png\" alt=\"Github\" title=\"RDD文件流\"></p>\n","categories":[],"tags":[]},{"title":"Spark","url":"https://weizhaome.github.io/2018/09/17/Hive/","content":"<h1 id=\"Spark-笔记\"><a href=\"#Spark-笔记\" class=\"headerlink\" title=\"Spark 笔记\"></a>Spark 笔记</h1><ul>\n<li><h2 id=\"Spark架构设计\"><a href=\"#Spark架构设计\" class=\"headerlink\" title=\"Spark架构设计\"></a>Spark架构设计</h2><strong>基本概念：</strong></li>\n</ul>\n<p>1.RDD:弹性分布式数据集，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型。<br>2.Executor:运行在工作节点（Worker Node）上的一个进程，负责运行任务，并为应用程序存储数据。<br>3.DAG:有向无环图（Driected Acyclic Graph）,反映RDD之间的依赖关系（宽依赖和窄依赖）。<br>4.应用：用户编写的Spark应用程序。<br>5.任务：运行在Exector的工作单元。<br>6.作业：一个作业包含多个RDD及作用在相应RDD上的各种操作。<br>7.阶段：是作业的基本调度单位，一个作业会分为多组任务，每组任务被成为”阶段“或者“任务集”。</p>\n<p><strong>架构图</strong></p>\n<p><img src=\"/images/Spark-1.png\" alt=\"Github\" title=\"Spark\"></p>\n<p><strong>运行基本流程</strong></p>\n<p>1.当一个Spark应用被提交时，首先为这个应用构建起基本的运行环境，即由任务控制节点（Driver）创建一个SparkContext,由SparkContext负责和资源管理器（Cluster Manager）的通信以及资源的申请、任务的分配和监控等。SparkContext会向资源管理器注册并申请运行Executor的资源。<br>2.资源管理器为Executor分配资源，并启动Executor进程，Executor运行情况将随着“心跳”发送给资源管理器。<br>3.SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAG调度器进行解析，将DAG图分解为多个“阶段”（每个阶段就是一个“任务集”），并且计算出各个阶段之间的依赖关系，然后把一个个“任务集”提交给底层的任务调度器进行处理，Executor向SparkContext申请任务，任务调度器将任务分发给Executor运行。同时，SparkContext将应用程序代码发给Executor。<br>4.任务在Executor上运行，把执行结构反馈给任务调度器，然后反馈给DAG调度器，运行完毕后写入数据并释放所有资源。</p>\n<p>参照：<a href=\"http://dblab.xmu.edu.cn/blog/1709-2/\" target=\"_blank\" rel=\"noopener\">http://dblab.xmu.edu.cn/blog/1709-2/</a></p>\n","categories":[],"tags":["Spark"]},{"title":"Hadoop 笔记","url":"https://weizhaome.github.io/2018/09/16/Hadoop-笔记/","content":"<h1 id=\"Hadoop基础\"><a href=\"#Hadoop基础\" class=\"headerlink\" title=\"Hadoop基础\"></a>Hadoop基础</h1><ul>\n<li><h2 id=\"是什么\"><a href=\"#是什么\" class=\"headerlink\" title=\"是什么\"></a>是什么</h2><blockquote>\n<p>Hadoop是Apache开源组织的一个分布式计算开源框架（<a href=\"http://hadoop.apache.org/)，\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/)，</a> 能够实现集群中对海量数据进行分布式计算。</p>\n</blockquote>\n</li>\n<li><h2 id=\"Hadoop架构\"><a href=\"#Hadoop架构\" class=\"headerlink\" title=\"Hadoop架构\"></a>Hadoop架构</h2><img src=\"/images/Hadoop.png\" alt=\"Github\"></li>\n<li>HDFS: 提供对应用程序数据高吞吐量访问的分布式文件系统。</li>\n<li>YARN: 作业调度和集群资源管理的框架</li>\n<li>MapReduce: 基于YARN的大型数据集并行处理系统，实现分布式计算。</li>\n<li>Others:利用YARN的资源管理功能实现其他的数据处理方式。<h3 id=\"HDFS的关键元素\"><a href=\"#HDFS的关键元素\" class=\"headerlink\" title=\"HDFS的关键元素\"></a>HDFS的关键元素</h3></li>\n</ul>\n<ol>\n<li>Block：将一个文件进行分块，通常是64M。一个大文件会被拆分成一个个的块，然后存储于不同的机器。如果一个文件少于Block大小，那么实际占用的空间为其文件的大小。</li>\n<li>NameNode: 保存整个文件系统的目录信息、文件信息及分块信息，这是由唯一一台主机专门保存，当然这台主机如果出错，NameNode就失效了。在Hadoop2.*开始支持activity-standy模式—-如果主NameNode失效，启动备用主机运行NameNode。</li>\n<li>DataNode: 保存具体的block数据,负责数据的读写操作和复制操作,DataNode启动时会向NameNode报告当前存储的数据块信息，后续也会定时报告修改信息,DataNode之间会进行通信，复制数据块，保证数据的冗余性。</li>\n<li>Secondary NameNode：定时与NameNode进行同步（定期合并文件系统镜像和编辑日&amp;#x#x5FD7;，然后把合并后的传给NameNode，替换其镜像，并清空编辑日志，类似于CheckPoint机制），但NameNode失效后仍需要手工将其设置成主机。<h3 id=\"YARN的组件以及架构\"><a href=\"#YARN的组件以及架构\" class=\"headerlink\" title=\"YARN的组件以及架构\"></a>YARN的组件以及架构</h3><h4 id=\"YARN主要由以下几个组件组成：\"><a href=\"#YARN主要由以下几个组件组成：\" class=\"headerlink\" title=\"YARN主要由以下几个组件组成：\"></a>YARN主要由以下几个组件组成：</h4><blockquote>\n<ol>\n<li>ResourceManager：Global（全局）的进程 </li>\n<li>NodeManager：运行在每个节点上的进程</li>\n<li>ApplicationMaster：Application-specific（应用级别）的进程</li>\n</ol>\n<ul>\n<li><em>Scheduler：是ResourceManager的一个组件</em></li>\n<li><em>Container：节点上一组CPU和内存资源</em></li>\n</ul>\n</blockquote>\n</li>\n</ol>\n<ul>\n<li>YARN的架构图及流程</li>\n</ul>\n<p><img src=\"/images/YARN.png\" alt=\"Github\" title=\"YARN\"></p>\n<p>1.客户端程序向ResourceManager提交应用并请求一个ApplicationMaster实例</p>\n<p>2.ResourceManager找到可以运行一个Container的NodeManager，并在这个Container中启动ApplicationMaster实例</p>\n<p>3.ApplicationMaster向ResourceManager进行注册，注册之后客户端就可以查询ResourceManager获得自己ApplicationMaster的详细信息，以后就可以和自己的ApplicationMaster直接交互了</p>\n<p>4.在平常的操作过程中，ApplicationMaster根据resource-request协议向ResourceManager发送resource-request请求</p>\n<p>5.当Container被成功分配之后，ApplicationMaster通过向NodeManager发送container-launch-specification信息来启动Container， container-launch-specification信息包含了能够让Container和ApplicationMaster交流所需要的资料</p>\n<p>6.应用程序的代码在启动的Container中运行，并把运行的进度、状态等信息通过application-specific协议发送给ApplicationMaster</p>\n<p>7.在应用程序运行期间，提交应用的客户端主动和ApplicationMaster交流获得应用的运行状态、进度更新等信息，交流的协议也是application-specific协议</p>\n<p>8.一但应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster向ResourceManager取消注册然后关闭，用到所有的Container也归还给系统<br>详情请见：<a href=\"https://blog.csdn.net/suifeng3051/article/details/49486927\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/suifeng3051/article/details/49486927</a></p>\n<h3 id=\"MapReduce主要过程\"><a href=\"#MapReduce主要过程\" class=\"headerlink\" title=\"MapReduce主要过程\"></a>MapReduce主要过程</h3><p>MapReduce把一个任务拆分成了多个小任务，并把子任务分配到多台计算机上进行工作。最终，每台计算机上的计算结果会被搜集起来并合并成最终的结果。在Map阶段将数据集的键值对映射为另一组键值对。Reduce阶段得到Map的输出，并把具有相同键的数据合并成一个更小的键值对数据集。<br><img src=\"/images/MapReduce.png\" alt=\"Github\" title=\"MapReduce\"></p>\n<ul>\n<li>Input Phase: 使用一个Record Reader将输入文件中的每一条数据转换为键值对的形式，并把这些处理好的数据发送给Mapper。</li>\n<li>Map: Map是是用户自定义的一个函数，此函数接收一系列的键值对数据并对它们进行处理，最后生成0个或多个键值对数据。</li>\n<li>Intermediate Keys: 由mapper生成的键值对数据被称为中间状态的键值对。</li>\n<li>Shuffle and Sort: Reducer任务通常以Shuffle（搅动）和Sort（排序）开始。程序把分好组的键值对数据下载到本机，Reducer会在本机进行运行。这些独立的键值对数据会按照键值进行排序并形成一个较大的数据序列，数据序列中键值相等的键值对数据会被分在相同的一组，这样易于在Reducer任务中进行迭代操作。</li>\n<li>Reducer:Reducer任务把分好组的键值对数据作为输入，并且对每一个键值对都执行Reducer函数。在这个阶段，程序会以不同的方式对数据进行合并、筛选。一旦执行完毕，Reducer会生成0个或多个键值对数据，并提供给最后一个处理步骤。</li>\n<li>Output Phase: 在输出阶段，通过record writer把从Reducer函数输出的键值对数据按照一定的格式写入到文件中。<br>参考：<a href=\"https://www.jianshu.com/p/6162b787a428\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/6162b787a428</a></li>\n</ul>\n","categories":[],"tags":["BigData"]},{"title":"Tags","url":"https://weizhaome.github.io/tag/index.html","content":"<p>Hadoop<br>Spark</p>\n","categories":[],"tags":[]},{"title":"About Me","url":"https://weizhaome.github.io/about/index.html","content":"<ul>\n<li><strong>基本信息</strong></li>\n</ul>\n<blockquote>\n<p>姓名：赵伟  </p>\n</blockquote>\n<blockquote>\n<p>性别：女</p>\n</blockquote>\n<blockquote>\n<p>外语水平：CET6</p>\n</blockquote>\n<blockquote>\n<p>邮箱： <a href=\"mailto:zhao_wei@tju.edu.cn\" target=\"_blank\" rel=\"noopener\">zhao_wei@tju.edu.cn</a></p>\n</blockquote>\n<blockquote>\n<p>硕士就读于天津大学（985）计算机技术专业（2017至今）</p>\n</blockquote>\n<blockquote>\n<p>本科就读于东北林业大学（211）软件工程专业（2013-2017）</p>\n</blockquote>\n<ul>\n<li><strong>论文成果</strong></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">时间</th>\n<th style=\"text-align:center\">论文名称</th>\n<th style=\"text-align:center\">排序</th>\n<th style=\"text-align:center\">刊物等级</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">   2017.12</td>\n<td style=\"text-align:center\">UltraPse: A Universal and Extensible Software Platform for Representing Biological Sequences</td>\n<td style=\"text-align:center\">二作</td>\n<td style=\"text-align:center\">SCI三区</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">   2018.09</td>\n<td style=\"text-align:center\">A brief review on software tools in generating Chou’s pseudo-factor representations for all types of biological sequences</td>\n<td style=\"text-align:center\">一作</td>\n<td style=\"text-align:center\">SCI四区</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">   2019.02</td>\n<td style=\"text-align:center\">Predicting protein sub-Golgi locations by combining functional domain enrichment scores with pseudo-amino acid compositions</td>\n<td style=\"text-align:center\">一作</td>\n<td style=\"text-align:center\">SCI四区在投</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><strong>所获证书</strong></li>\n</ul>\n<blockquote>\n<p>1、2016-2017学年： 获得“三好学生”荣誉称号；“国家励志”奖学金；</p>\n</blockquote>\n<blockquote>\n<p>2、2015-2016学年： 获得“优秀团员”荣誉称号；</p>\n</blockquote>\n<blockquote>\n<p>3、2014-2015学年：“牵手”奖学金；获得校级“优秀学生干部”荣誉称号。</p>\n</blockquote>\n<ul>\n<li><strong>学生干部任职情况</strong></li>\n</ul>\n<blockquote>\n<p>1、2015.09-2016.09 担任东北林业大学勤工助学中心部长，组织东北林业大学家庭经济困难学生技能培训并取得圆满成功；</p>\n</blockquote>\n<blockquote>\n<p>2、2013.09-2017.06 担任寝室长，带领我们寝室成为校“优秀寝室”。</p>\n</blockquote>\n<ul>\n<li><strong> 社会实践经历</strong></li>\n</ul>\n<blockquote>\n<p>1、2018.03-2018.07，在天津大学担任C++课程助教，协助老师完成学生上机实验并在过程中解决学生遇到的问题；</p>\n</blockquote>\n<blockquote>\n<p>2、2015.09-2016.09，在东北林业大学教育超市丹青分店担任收银员，让自己的耐心和细心都有所提高。</p>\n</blockquote>\n","categories":[],"tags":[]}]